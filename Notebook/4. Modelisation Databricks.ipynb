{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install azure-storage-blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.mount(\n",
    "  source = \"blob_container\",\n",
    "  mount_point = \"/mnt/p11-mount\",\n",
    "  extra_configs = {\"account-key\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/mnt/p11-mount/data\"\n",
    "PATH_Data = PATH+'/Test'\n",
    "PATH_Result = PATH+'/Results'\n",
    "\n",
    "print('PATH:        '+\\\n",
    "      PATH+'\\nPATH_Data:   '+\\\n",
    "      PATH_Data+'\\nPATH_Result: '+PATH_Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Traitement des données\n",
    "### 2.1 Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Ajout d'une colonne 'label' à notre DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous ajoutons une nouvelle colonne à notre DataFrame 'images'. Cette colonne, appelée 'label', est créée en extrayant le nom du dossier contenant chaque image à partir du chemin de l'image.\n",
    "\n",
    "Voici comment cela fonctionne :\n",
    "\n",
    "1. Nous divisons le chemin de chaque image en plusieurs parties en utilisant le caractère '/' comme séparateur.\n",
    "2. Nous prenons l'avant-dernière partie, qui correspond au nom du dossier contenant l'image.\n",
    "3. Nous ajoutons cette information comme une nouvelle colonne dans notre DataFrame.\n",
    "\n",
    "Après avoir ajouté la colonne 'label', nous vérifions son ajout en affichant le schéma du DataFrame et les cinq premières lignes des colonnes 'path' et 'label'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\n",
    "print(images.printSchema())\n",
    "print(images.select('path','label').show(5,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Préparation du modèle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNetV2(weights='imagenet',\n",
    "                    include_top=True,\n",
    "                    input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Diffusion des poids du modèle\n",
    "\n",
    "Nous utilisons la fonction `broadcast` de Spark pour diffuser les poids du modèle à tous les nœuds du cluster. Cela permet d'assurer que tous les nœuds ont une copie des poids du modèle, ce qui est nécessaire pour effectuer des prédictions ou des mises à jour de modèle sur les données distribuées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brodcast_weights = spark.broadcast(new_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Définition du processus de chargement des images <br/> et application de leur featurisation à travers l'utilisation de pandas UDF\n",
    "<u>La séquence d'appels est organisée comme suit</u> :\n",
    "\n",
    "- Pandas UDF\n",
    "  - Appliquer la featurisation à une série d'images pd.Series\n",
    "   - Effectuer le prétraitement d'une image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(weights='imagenet',\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(brodcast_weights.value)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Définition du processus de chargement des images <br/> et application de leur featurisation à travers l'utilisation de pandas UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>La séquence d'appels est organisée comme suit</u> :\n",
    "\n",
    "- Pandas UDF\n",
    "  - Appliquer la featurisation à une série d'images pd.Series\n",
    "   - Effectuer le prétraitement d'une image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Exécutions des actions d'extractions de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = images.repartition(24).select(col(\"path\"),\n",
    "                                            col(\"label\"),\n",
    "                                            featurize_udf(\"content\").alias(\"features\")\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.write.mode(\"overwrite\").parquet(PATH_Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant effectuer une **Analyse en Composantes Principales (PCA)** sur notre ensemble de données. \n",
    "\n",
    "#### Préparation des données\n",
    "\n",
    "Pour commencer, nous définissons une **fonction utilisateur (UDF)** qui convertit un tableau en un vecteur. C'est une étape nécessaire car la fonction PCA de PySpark attend des données sous forme de vecteurs.\n",
    "\n",
    "Ensuite, nous chargeons notre ensemble de données à partir d'un fichier **Parquet**. Parquet est un format de fichier optimisé pour le stockage en colonnes, ce qui est idéal pour les grands ensembles de données.\n",
    "\n",
    "Une fois les données chargées, nous convertissons une colonne de l'ensemble de données, qui est un tableau de nombres à virgule flottante, en un vecteur à l'aide de l'UDF que nous avons définie.\n",
    "\n",
    "#### Application de la PCA\n",
    "\n",
    "Maintenant, nous sommes prêts à appliquer la PCA à la colonne de vecteurs. Nous spécifions que nous voulons deux composantes principales avec le paramètre `k`.\n",
    "\n",
    "Après avoir appliqué la PCA, nous supprimons la colonne de vecteurs originale de l'ensemble de données. Cela nous permet d'économiser de l'espace, car cette colonne n'est plus nécessaire.\n",
    "\n",
    "#### Conversion en DataFrame Pandas\n",
    "\n",
    "Enfin, nous convertissons l'ensemble de données Spark en un **DataFrame Pandas**. Cela nous permet d'utiliser facilement les données avec des bibliothèques Python qui s'attendent à des DataFrames Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "# Define a UDF to convert array to vector\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "# Load the data\n",
    "df = spark.read.parquet(PATH_Result)\n",
    "\n",
    "# Convert the array of floats to a vector\n",
    "df = df.withColumn(\"features_vec\", list_to_vector_udf(df[\"features\"]))\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(k=2, inputCol=\"features_vec\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(df)\n",
    "result = model.transform(df)\n",
    "result = result.drop('features_vec')\n",
    "result = result.drop('features')\n",
    "\n",
    "# Convert the Spark DataFrame to a Pandas DataFrame\n",
    "result_pd = result.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Affichage Resultats et sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "result_pd.to_csv(os.path.join(PATH_Result, 'azure_result.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
